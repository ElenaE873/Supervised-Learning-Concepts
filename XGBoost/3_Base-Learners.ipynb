{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cefbe2",
   "metadata": {},
   "source": [
    "### Objective or Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a86afa",
   "metadata": {},
   "source": [
    "- QUANTIFIES how far off a prediction is from an actual result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85b39f",
   "metadata": {},
   "source": [
    "The aim with a model is ideally to minimize this error (or loss function) for ALL of the data points that we pass through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec738c",
   "metadata": {},
   "source": [
    "### Common Loss Functions for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7327d72",
   "metadata": {},
   "source": [
    "Regression Problems:\n",
    "- reg:linear\n",
    "\n",
    "Classification Problems:\n",
    "- reg:logistic  (just decision, not probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34945c",
   "metadata": {},
   "source": [
    "### Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9cd9c",
   "metadata": {},
   "source": [
    "XGBoost involves creating a meta-model that is composed of many indiv. models that combine to give a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71e040",
   "metadata": {},
   "source": [
    "THE INDIVIDUAL MODELS ARE the BASE LEARNERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0011bd6",
   "metadata": {},
   "source": [
    "### Base Learners in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e10ec",
   "metadata": {},
   "source": [
    "### Two kinds of base learners:\n",
    "- tree and linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8684fca",
   "metadata": {},
   "source": [
    "Linear Based Learners:\n",
    "- sum of linear terms\n",
    "- boosted model is weighted sum of linear models\n",
    "- rarely used\n",
    "\n",
    "Tree Based Learners:\n",
    "- Decision Tree\n",
    "- Boosted Model is weighted sum of decision trees (nonlinear)\n",
    "- almost exclusively used in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6765454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "#regressor object\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9517d1",
   "metadata": {},
   "source": [
    "#### Linear Base Learners, need the Learning API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeace7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after train/test split\n",
    "#need to turn train/test splits into DMatrixes\n",
    "\n",
    "#this is required by the Learning API\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_train = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "#specifying the base learner we want\n",
    "params = {\"booster\":\"gblinear\", \n",
    "          \"objective\":\"reg:linear\"}\n",
    "\n",
    "xg_reg = xgb.train(params = params,\n",
    "                  dtrain=DM_train,\n",
    "                  num_boost_round=10)\n",
    "\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6339158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing cross-validation with 5 boosting rounds and \"rmse as metric\"\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, \n",
    "                    num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final round boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec5d9d",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13f3bf",
   "metadata": {},
   "source": [
    "Is it a form of control over the model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9bb629",
   "metadata": {},
   "source": [
    "#### Regularization parameters in XGBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f3d9f",
   "metadata": {},
   "source": [
    "- **gamma**: minimum loss reduction allowed for a split to occur\n",
    "- **alpha**: L1 regularization on leaf weights, larger values mean more regularization\n",
    "- **lambda**: L2 regularization on leaf weights, smoother than L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 regularization in XGBoost, example\n",
    "\n",
    "#define X and y\n",
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "\n",
    "#create the maxtrix\n",
    "DMatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "#defining params\n",
    "params={\"objective\":\"reg:linar\", \"max_depth\":4}\n",
    "\n",
    "#creating three alpha L1 values\n",
    "l1_params = [1,10,100]\n",
    "\n",
    "#storing these rmse for each alpha values in a list\n",
    "rmses_l1=[]\n",
    "\n",
    "#loop to iterate over each entry in l1_params list and do the following:\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg\n",
    "    cv_results = xgb.cv(dtrain=DMatrix, params=params, nfold=4,\n",
    "                       num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    rmses_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
    "    \n",
    "print(\"Best rmse as a function of L1:\")\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"l1\", \"rmse\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
